{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from apyori import apriori\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Dataset/Dataset_EditActions.csv')\n",
    "df = df.drop(['Architecture', 'Reproducible?', 'Code Snippet Present?', 'System Configuration Present?', 'Data Description Present?', 'Framework'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Training Bug with value T, Model bug with value M, API bug with value A, Tensor & Input Bug with value I, and GPU Bug with value G\n",
    "df.loc[df['Type of Bug'] == 'Training Bug', 'Type of Bug'] = 'T'\n",
    "df.loc[df['Type of Bug'] == 'Model Bug', 'Type of Bug'] = 'M'\n",
    "df.loc[df['Type of Bug'] == 'API Bug', 'Type of Bug'] = 'A'\n",
    "df.loc[df['Type of Bug'] == 'Tensor and Input Bug', 'Type of Bug'] = 'I'\n",
    "df.loc[df['Type of Bug'] == 'GPU Bug', 'Type of Bug'] = 'G'\n",
    "df.loc[df['Type of Bug'] == 'Mixed Bug', 'Type of Bug'] = 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map values from new columns as the following:\n",
    "# Input Data Generation: D \n",
    "# Neural Network Definition: N\n",
    "# Obsolete Parameter Removal: O\n",
    "# Framework Migration: F\n",
    "# Dataset Procurement: P\n",
    "# Models and Tokenizers: M\n",
    "# Logging: L\n",
    "# Import Addition and Dependency Resolution: R\n",
    "# Compiler Error Resolution: C\n",
    "# Hyperparameter Initialization: H\n",
    "\n",
    "# Replace all 1's with the corresponding letter\n",
    "df['Input Data Generation'] = df['Input Data Generation'].replace(1, 'D')\n",
    "df['Neural Network Definition'] = df['Neural Network Definition'].replace(1, 'N')\n",
    "df['Obsolete Parameter Removal'] = df['Obsolete Parameter Removal'].replace(1, 'O')\n",
    "df['Framework Migration'] = df['Framework Migration'].replace(1, 'F')\n",
    "df['Dataset Procurement'] = df['Dataset Procurement'].replace(1, 'P')\n",
    "df['Downloading Models and Tokenizers'] = df['Downloading Models and Tokenizers'].replace(1, 'M')\n",
    "df['Logging'] = df['Logging'].replace(1, 'L')\n",
    "df['Import Addition and Dependency Resolution'] = df['Import Addition and Dependency Resolution'].replace(1, 'R')\n",
    "df['Compiler Error Resolution'] = df['Compiler Error Resolution'].replace(1, 'C')\n",
    "df['Hyperparameter Initialization'] = df['Hyperparameter Initialization'].replace(1, 'H')\n",
    "\n",
    "\n",
    "df['Input Data Generation'] = df['Input Data Generation'].replace(0, '')\n",
    "df['Neural Network Definition'] = df['Neural Network Definition'].replace(0, '')\n",
    "df['Obsolete Parameter Removal'] = df['Obsolete Parameter Removal'].replace(0, '')\n",
    "df['Framework Migration'] = df['Framework Migration'].replace(0, '')\n",
    "df['Dataset Procurement'] = df['Dataset Procurement'].replace(0, '')\n",
    "df['Downloading Models and Tokenizers'] = df['Downloading Models and Tokenizers'].replace(0, '')\n",
    "df['Logging'] = df['Logging'].replace(0, '')\n",
    "df['Import Addition and Dependency Resolution'] = df['Import Addition and Dependency Resolution'].replace(0, '')\n",
    "df['Compiler Error Resolution'] = df['Compiler Error Resolution'].replace(0, '')\n",
    "df['Hyperparameter Initialization'] = df['Hyperparameter Initialization'].replace(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bugs = df[df['Type of Bug'] == 'T']\n",
    "gpu_bugs = df[df['Type of Bug'] == 'G']\n",
    "api_bugs = df[df['Type of Bug'] == 'A']\n",
    "model_bugs = df[df['Type of Bug'] == 'M']\n",
    "tensor_bugs = df[df['Type of Bug'] == 'I']\n",
    "mixed_bugs = df[df['Type of Bug'] == 'X']\n",
    "\n",
    "training_transactions = []\n",
    "for i in range(0, len(training_bugs)):\n",
    "    training_transactions.append([str(training_bugs.values[i,j]) for j in range(0, len(training_bugs.columns)) if str(training_bugs.values[i,j]) != ''])\n",
    "\n",
    "gpu_transactions = []\n",
    "for i in range(0, len(gpu_bugs)):\n",
    "    gpu_transactions.append([str(gpu_bugs.values[i,j]) for j in range(0, len(gpu_bugs.columns)) if str(gpu_bugs.values[i,j]) != ''])\n",
    "\n",
    "api_transactions = []\n",
    "for i in range(0, len(api_bugs)):\n",
    "    api_transactions.append([str(api_bugs.values[i,j]) for j in range(0, len(api_bugs.columns)) if str(api_bugs.values[i,j]) != ''])\n",
    "\n",
    "model_transactions = []\n",
    "for i in range(0, len(model_bugs)):\n",
    "    model_transactions.append([str(model_bugs.values[i,j]) for j in range(0, len(model_bugs.columns)) if str(model_bugs.values[i,j]) != ''])\n",
    "\n",
    "tensor_transactions = []\n",
    "for i in range(0, len(tensor_bugs)):\n",
    "    tensor_transactions.append([str(tensor_bugs.values[i,j]) for j in range(0, len(tensor_bugs.columns)) if str(tensor_bugs.values[i,j]) != ''])\n",
    "\n",
    "mixed_transactions = []\n",
    "for i in range(0, len(mixed_bugs)):\n",
    "    mixed_transactions.append([str(mixed_bugs.values[i,j]) for j in range(0, len(mixed_bugs.columns)) if str(mixed_bugs.values[i,j]) != ''])\n",
    "\n",
    "transactions = []\n",
    "for i in range(0, len(df)):\n",
    "    transactions.append([str(df.values[i,j]) for j in range(0, len(df.columns)) if str(df.values[i,j]) != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "github_transactions = '../Dataset/ManualReproduction_Github_Bugs.csv'\n",
    "\n",
    "# Column headers that should be considered for the dynamic output\n",
    "considered_columns = ['D', 'N', 'H', 'R', 'L', 'O', 'C', 'P', 'M', 'V']\n",
    "\n",
    "# Function to process the CSV file and extract transactions dynamically\n",
    "def process_csv(file_path):\n",
    "    transactions = []\n",
    "    \n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        # Iterate over each row in the CSV\n",
    "        for row in reader:\n",
    "            # Collect the columns marked as '1'\n",
    "            marked_columns = [col for col in considered_columns if row[col] == '1']\n",
    "            \n",
    "            # Check if there are any marked columns for this row\n",
    "            if marked_columns:\n",
    "                if row['type'] == 'Training':\n",
    "                    # Create a transaction in the format ['T', 'D', 'F', 'R'] etc,.\n",
    "                    transaction = ['T'] + marked_columns\n",
    "                    training_transactions.append(transaction)\n",
    "                elif row['type'] == 'GPU':\n",
    "                    transaction = ['G'] + marked_columns\n",
    "                    gpu_transactions.append(transaction)\n",
    "                elif row['type'] == 'API':\n",
    "                    transaction = ['A'] + marked_columns\n",
    "                    api_transactions.append(transaction)\n",
    "                elif row['type'] == 'Model':\n",
    "                    transaction = ['M'] + marked_columns\n",
    "                    model_transactions.append(transaction)\n",
    "                elif row['type'] == 'Tensor and Input':\n",
    "                    transaction = ['I'] + marked_columns\n",
    "                    tensor_transactions.append(transaction)\n",
    "\n",
    "process_csv(github_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apriori_results(rules, character):\n",
    "    filtered_results = []\n",
    "\n",
    "    # Collect relevant data\n",
    "    for result in rules:\n",
    "        if character in result.items:\n",
    "            for stat in result.ordered_statistics:\n",
    "                if character in stat.items_base:\n",
    "                    filtered_results.append({\n",
    "                        \"items\": result.items,\n",
    "                        \"support\": result.support,\n",
    "                        \"antecedent\": stat.items_base,\n",
    "                        \"consequent\": stat.items_add,\n",
    "                        \"confidence\": stat.confidence,\n",
    "                        \"lift\": stat.lift\n",
    "                    })\n",
    "\n",
    "    # Sort the results by support first, then by confidence\n",
    "    filtered_results.sort(key=lambda x: (x[\"support\"], x[\"confidence\"]), reverse=True)\n",
    "\n",
    "    # Print sorted results\n",
    "    for res in filtered_results:\n",
    "        items = \", \".join(res[\"items\"])\n",
    "        print(f\"Items: {{{items}}}\")\n",
    "        print(f\"Support: {res['support']:.4f}\")\n",
    "        print(\"Association Rules:\")\n",
    "        antecedent = \", \".join(res[\"antecedent\"])\n",
    "        consequent = \", \".join(res[\"consequent\"])\n",
    "        print(f\"  {{{antecedent}}} => {{{consequent}}}\")\n",
    "        print(f\"Confidence: {res['confidence']:.4f}\")\n",
    "        print(f\"Lift: {res['lift']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: {T, D}\n",
      "Support: 0.5625\n",
      "Association Rules:\n",
      "  {T} => {D}\n",
      "Confidence: 0.5625\n",
      "Lift: 1.0000\n",
      "Items: {R, T}\n",
      "Support: 0.4583\n",
      "Association Rules:\n",
      "  {T} => {R}\n",
      "Confidence: 0.4583\n",
      "Lift: 1.0000\n",
      "Items: {C, T}\n",
      "Support: 0.3750\n",
      "Association Rules:\n",
      "  {T} => {C}\n",
      "Confidence: 0.3750\n",
      "Lift: 1.0000\n",
      "Items: {T, P}\n",
      "Support: 0.3542\n",
      "Association Rules:\n",
      "  {T} => {P}\n",
      "Confidence: 0.3542\n",
      "Lift: 1.0000\n",
      "Items: {T, H}\n",
      "Support: 0.3333\n",
      "Association Rules:\n",
      "  {T} => {H}\n",
      "Confidence: 0.3333\n",
      "Lift: 1.0000\n",
      "Items: {O, T}\n",
      "Support: 0.2292\n",
      "Association Rules:\n",
      "  {T} => {O}\n",
      "Confidence: 0.2292\n",
      "Lift: 1.0000\n",
      "Items: {F, T}\n",
      "Support: 0.1875\n",
      "Association Rules:\n",
      "  {T} => {F}\n",
      "Confidence: 0.1875\n",
      "Lift: 1.0000\n",
      "Items: {L, T}\n",
      "Support: 0.1458\n",
      "Association Rules:\n",
      "  {T} => {L}\n",
      "Confidence: 0.1458\n",
      "Lift: 1.0000\n"
     ]
    }
   ],
   "source": [
    "get_apriori_results(apriori(training_transactions, max_length = 2), 'T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: {M, H}\n",
      "Support: 0.5122\n",
      "Association Rules:\n",
      "  {M} => {H}\n",
      "Confidence: 0.5122\n",
      "Lift: 1.0000\n",
      "Items: {M, P}\n",
      "Support: 0.4390\n",
      "Association Rules:\n",
      "  {M} => {P}\n",
      "Confidence: 0.4390\n",
      "Lift: 1.0000\n",
      "Items: {C, M}\n",
      "Support: 0.4146\n",
      "Association Rules:\n",
      "  {M} => {C}\n",
      "Confidence: 0.4146\n",
      "Lift: 1.0000\n",
      "Items: {R, M}\n",
      "Support: 0.4146\n",
      "Association Rules:\n",
      "  {M} => {R}\n",
      "Confidence: 0.4146\n",
      "Lift: 1.0000\n",
      "Items: {N, M}\n",
      "Support: 0.3659\n",
      "Association Rules:\n",
      "  {M} => {N}\n",
      "Confidence: 0.3659\n",
      "Lift: 1.0000\n",
      "Items: {M, D}\n",
      "Support: 0.3171\n",
      "Association Rules:\n",
      "  {M} => {D}\n",
      "Confidence: 0.3171\n",
      "Lift: 1.0000\n",
      "Items: {V, M}\n",
      "Support: 0.2439\n",
      "Association Rules:\n",
      "  {M} => {V}\n",
      "Confidence: 0.2439\n",
      "Lift: 1.0000\n",
      "Items: {L, M}\n",
      "Support: 0.2195\n",
      "Association Rules:\n",
      "  {M} => {L}\n",
      "Confidence: 0.2195\n",
      "Lift: 1.0000\n",
      "Items: {O, M}\n",
      "Support: 0.1951\n",
      "Association Rules:\n",
      "  {M} => {O}\n",
      "Confidence: 0.1951\n",
      "Lift: 1.0000\n"
     ]
    }
   ],
   "source": [
    "get_apriori_results(apriori(model_transactions, max_length = 2), 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: {I, H}\n",
      "Support: 0.5517\n",
      "Association Rules:\n",
      "  {I} => {H}\n",
      "Confidence: 0.5517\n",
      "Lift: 1.0000\n",
      "Items: {D, I}\n",
      "Support: 0.5172\n",
      "Association Rules:\n",
      "  {I} => {D}\n",
      "Confidence: 0.5172\n",
      "Lift: 1.0000\n",
      "Items: {R, I}\n",
      "Support: 0.5172\n",
      "Association Rules:\n",
      "  {I} => {R}\n",
      "Confidence: 0.5172\n",
      "Lift: 1.0000\n",
      "Items: {C, I}\n",
      "Support: 0.4138\n",
      "Association Rules:\n",
      "  {I} => {C}\n",
      "Confidence: 0.4138\n",
      "Lift: 1.0000\n",
      "Items: {O, I}\n",
      "Support: 0.3448\n",
      "Association Rules:\n",
      "  {I} => {O}\n",
      "Confidence: 0.3448\n",
      "Lift: 1.0000\n",
      "Items: {L, I}\n",
      "Support: 0.3103\n",
      "Association Rules:\n",
      "  {I} => {L}\n",
      "Confidence: 0.3103\n",
      "Lift: 1.0000\n",
      "Items: {P, I}\n",
      "Support: 0.4138\n",
      "Association Rules:\n",
      "  {I} => {P}\n",
      "Confidence: 0.4138\n",
      "Lift: 1.0000\n",
      "Items: {V, I}\n",
      "Support: 0.1034\n",
      "Association Rules:\n",
      "  {I} => {V}\n",
      "Confidence: 0.1034\n",
      "Lift: 1.0000\n"
     ]
    }
   ],
   "source": [
    "get_apriori_results(apriori(tensor_transactions, max_length = 2), 'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: {A, R}\n",
      "Support: 0.3500\n",
      "Association Rules:\n",
      "  {A} => {R}\n",
      "Confidence: 0.3500\n",
      "Lift: 1.0000\n",
      "Items: {A, D}\n",
      "Support: 0.4000\n",
      "Association Rules:\n",
      "  {A} => {D}\n",
      "Confidence: 0.4000\n",
      "Lift: 1.0000\n",
      "Items: {A, H}\n",
      "Support: 0.4000\n",
      "Association Rules:\n",
      "  {A} => {H}\n",
      "Confidence: 0.4000\n",
      "Lift: 1.0000\n",
      "Items: {A, P}\n",
      "Support: 0.1500\n",
      "Association Rules:\n",
      "  {A} => {P}\n",
      "Confidence: 0.1500\n",
      "Lift: 1.0000\n",
      "Items: {C, A}\n",
      "Support: 0.1500\n",
      "Association Rules:\n",
      "  {A} => {C}\n",
      "Confidence: 0.1500\n",
      "Lift: 1.0000\n",
      "Items: {O, A}\n",
      "Support: 0.2500\n",
      "Association Rules:\n",
      "  {A} => {O}\n",
      "Confidence: 0.2500\n",
      "Lift: 1.0000\n",
      "Items: {A, L}\n",
      "Support: 0.2500\n",
      "Association Rules:\n",
      "  {A} => {L}\n",
      "Confidence: 0.2500\n",
      "Lift: 1.0000\n",
      "Items: {N, A}\n",
      "Support: 0.1000\n",
      "Association Rules:\n",
      "  {A} => {N}\n",
      "Confidence: 0.1000\n",
      "Lift: 1.0000\n",
      "Items: {A, V}\n",
      "Support: 0.1000\n",
      "Association Rules:\n",
      "  {A} => {V}\n",
      "Confidence: 0.1000\n",
      "Lift: 1.0000\n"
     ]
    }
   ],
   "source": [
    "get_apriori_results(apriori(api_transactions, max_length = 2), 'A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
